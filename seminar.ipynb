{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhrn5O-qUYZ"
      },
      "source": [
        "# Import and misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "meO-Mp9jiAFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c974feea-1f34-4b46-e2db-7f9e1cbafa2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 6.8 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 15.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 48.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 40.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 71.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 56.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 54.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 42.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 50.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 52.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 50.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 14.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=a019c6f7183e63906962b3c0ef3372d074f75f43f6d5d9f0dedf8397515f69f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.5\n"
          ]
        }
      ],
      "source": [
        "# Instal latest torch and torchaudio\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bbUpoArCqUYa"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Union, List, Callable, Optional\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "import pathlib\n",
        "import dataclasses\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchaudio\n",
        "from IPython import display as display_\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0ZrQXAWNtpEU"
      },
      "outputs": [],
      "source": [
        "SEED = 7\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812GwLfqqUYf"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1DuQIyRqUYf"
      },
      "source": [
        "In this notebook we will implement a model for finding a keyword in a stream.\n",
        "\n",
        "We will implement the version with CRNN because it is easy and improves the model. \n",
        "(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8PdhApeEh9pH"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class TaskConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_epochs: int = 20\n",
        "    n_mels: int = 40\n",
        "    cnn_out_channels: int = 8\n",
        "    kernel_size: Tuple[int, int] = (5, 20)\n",
        "    stride: Tuple[int, int] = (2, 8)\n",
        "    hidden_size: int = 64\n",
        "    gru_num_layers: int = 2\n",
        "    bidirectional: bool = False\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    max_window_length: int = 20 #change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1gPmE1h9pI"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y2N8zcx9MF1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c67ea14-1223-4a92-c920-9c88789b9890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-06 19:35:10--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.195.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.195.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1489096277 (1.4G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   1.39G   233MB/s    in 5.9s    \n",
            "\n",
            "2022-11-06 19:35:16 (240 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "!mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "12wBTK0mNUsG"
      },
      "outputs": [],
      "source": [
        "class SpeechCommandDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform: Optional[Callable] = None,\n",
        "        path2dir: str = None,\n",
        "        keywords: Union[str, List[str]] = None,\n",
        "        csv: Optional[pd.DataFrame] = None\n",
        "    ):        \n",
        "        self.transform = transform\n",
        "\n",
        "        if csv is None:\n",
        "            path2dir = pathlib.Path(path2dir)\n",
        "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
        "            \n",
        "            all_keywords = [\n",
        "                p.stem for p in path2dir.glob('*')\n",
        "                if p.is_dir() and not p.stem.startswith('_')\n",
        "            ]\n",
        "\n",
        "            triplets = []\n",
        "            for keyword in all_keywords:\n",
        "                paths = (path2dir / keyword).rglob('*.wav')\n",
        "                if keyword in keywords:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
        "                else:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
        "            \n",
        "            self.csv = pd.DataFrame(\n",
        "                triplets,\n",
        "                columns=['path', 'keyword', 'label']\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.csv = csv\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        instance = self.csv.iloc[index]\n",
        "\n",
        "        path2wav = instance['path']\n",
        "        wav, sr = torchaudio.load(path2wav)\n",
        "        wav = wav.sum(dim=0)\n",
        "        \n",
        "        if self.transform:\n",
        "            wav = self.transform(wav)\n",
        "\n",
        "        return {\n",
        "            'wav': wav,\n",
        "            'keywors': instance['keyword'],\n",
        "            'label': instance['label']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-1rVkT81Pk90"
      },
      "outputs": [],
      "source": [
        "dataset = SpeechCommandDataset(\n",
        "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DFwhAXdfQLIA",
        "outputId": "d64b66cb-4bf8-40b7-84e9-3968a6ab3161"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               path keyword  label\n",
              "44399    speech_commands/down/ccfd721c_nohash_0.wav    down      0\n",
              "50708   speech_commands/three/8549f25d_nohash_2.wav   three      0\n",
              "21470     speech_commands/bed/1aed7c6d_nohash_0.wav     bed      0\n",
              "11826     speech_commands/two/d9e9f554_nohash_2.wav     two      0\n",
              "2772   speech_commands/marvin/c103a2d5_nohash_0.wav  marvin      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6282a886-81ea-420f-9a6c-3d3b344c848f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>keyword</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44399</th>\n",
              "      <td>speech_commands/down/ccfd721c_nohash_0.wav</td>\n",
              "      <td>down</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50708</th>\n",
              "      <td>speech_commands/three/8549f25d_nohash_2.wav</td>\n",
              "      <td>three</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21470</th>\n",
              "      <td>speech_commands/bed/1aed7c6d_nohash_0.wav</td>\n",
              "      <td>bed</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11826</th>\n",
              "      <td>speech_commands/two/d9e9f554_nohash_2.wav</td>\n",
              "      <td>two</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2772</th>\n",
              "      <td>speech_commands/marvin/c103a2d5_nohash_0.wav</td>\n",
              "      <td>marvin</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6282a886-81ea-420f-9a6c-3d3b344c848f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6282a886-81ea-420f-9a6c-3d3b344c848f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6282a886-81ea-420f-9a6c-3d3b344c848f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset.csv.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxfDJw1qUYi"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dkmkxPWQqUYe"
      },
      "outputs": [],
      "source": [
        "class AugsCreation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.background_noises = [\n",
        "            'speech_commands/_background_noise_/white_noise.wav',\n",
        "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
        "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
        "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
        "            'speech_commands/_background_noise_/pink_noise.wav',\n",
        "            'speech_commands/_background_noise_/running_tap.wav'\n",
        "        ]\n",
        "\n",
        "        self.noises = [\n",
        "            torchaudio.load(p)[0].squeeze()\n",
        "            for p in self.background_noises\n",
        "        ]\n",
        "\n",
        "    def add_rand_noise(self, audio):\n",
        "\n",
        "        # randomly choose noise\n",
        "        noise_num = torch.randint(low=0, high=len(\n",
        "            self.background_noises), size=(1,)).item()\n",
        "        noise = self.noises[noise_num]\n",
        "\n",
        "        noise_level = torch.Tensor([1])  # [0, 40]\n",
        "\n",
        "        noise_energy = torch.norm(noise)\n",
        "        audio_energy = torch.norm(audio)\n",
        "        alpha = (audio_energy / noise_energy) * \\\n",
        "            torch.pow(10, -noise_level / 20)\n",
        "\n",
        "        start = torch.randint(\n",
        "            low=0,\n",
        "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
        "            size=(1,)\n",
        "        ).item()\n",
        "        noise_sample = noise[start: start + audio.size(0)]\n",
        "\n",
        "        audio_new = audio + alpha * noise_sample\n",
        "        audio_new.clamp_(-1, 1)\n",
        "        return audio_new\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
        "        augs = [\n",
        "            lambda x: x,\n",
        "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
        "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
        "            lambda x: self.add_rand_noise(x)\n",
        "        ]\n",
        "\n",
        "        return augs[aug_num](wav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ClWThxyYh9pM"
      },
      "outputs": [],
      "source": [
        "# indexes = torch.randperm(len(dataset))\n",
        "indexes = torch.load('indexes.pt')\n",
        "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
        "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
        "\n",
        "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
        "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PDPLht5fqUYe"
      },
      "outputs": [],
      "source": [
        "# Sample is a dict of utt, word and label\n",
        "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
        "val_set = SpeechCommandDataset(csv=val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbPDqd6qUYj"
      },
      "source": [
        "### Sampler for oversampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rfnjRKo2qUYj"
      },
      "outputs": [],
      "source": [
        "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
        "\n",
        "def get_sampler(target):\n",
        "    class_sample_count = np.array(\n",
        "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.float()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UM8gLmHeqUYj"
      },
      "outputs": [],
      "source": [
        "train_sampler = get_sampler(train_set.csv['label'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lyBqbxp0h9pO"
      },
      "outputs": [],
      "source": [
        "class Collator:\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        wavs = []\n",
        "        labels = []    \n",
        "\n",
        "        for el in data:\n",
        "            wavs.append(el['wav'])\n",
        "            labels.append(el['label'])\n",
        "\n",
        "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
        "        wavs = pad_sequence(wavs, batch_first=True)    \n",
        "        labels = torch.Tensor(labels).long()\n",
        "        return wavs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8G9xPRVqUYk"
      },
      "source": [
        "###  Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6wGBMcQiqUYk"
      },
      "outputs": [],
      "source": [
        "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlsn6cpqUYk"
      },
      "source": [
        "### Creating MelSpecs on GPU for speeeed: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pRXMt6it56fW"
      },
      "outputs": [],
      "source": [
        "class LogMelspec:\n",
        "\n",
        "    def __init__(self, is_train, config):\n",
        "        # with augmentations\n",
        "        if is_train:\n",
        "            self.melspec = nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(\n",
        "                    sample_rate=config.sample_rate,\n",
        "                    n_fft=400,\n",
        "                    win_length=400,\n",
        "                    hop_length=160,\n",
        "                    n_mels=config.n_mels\n",
        "                ),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
        "            ).to(config.device)\n",
        "\n",
        "        # no augmentations\n",
        "        else:\n",
        "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_fft=400,\n",
        "                win_length=400,\n",
        "                hop_length=160,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # already on device\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Pqkz4_gn8BiF"
      },
      "outputs": [],
      "source": [
        "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
        "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoAxmihY8yxr"
      },
      "source": [
        "### Quality measurment functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "euwD1UyuqUYk"
      },
      "outputs": [],
      "source": [
        "# FA - true: 0, model: 1\n",
        "# FR - true: 1, model: 0\n",
        "\n",
        "def count_FA_FR(preds, labels):\n",
        "    FA = torch.sum(preds[labels == 0])\n",
        "    FR = torch.sum(labels[preds == 0])\n",
        "    \n",
        "    # torch.numel - returns total number of elements in tensor\n",
        "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YHBUrkT1qUYk"
      },
      "outputs": [],
      "source": [
        "def get_au_fa_fr(probs, labels):\n",
        "    sorted_probs, _ = torch.sort(probs)\n",
        "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "        \n",
        "    FAs, FRs = [], []\n",
        "    for prob in sorted_probs:\n",
        "        preds = (probs >= prob) * 1\n",
        "        FA, FR = count_FA_FR(preds, labels)        \n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "    # plt.plot(FAs, FRs)\n",
        "    # plt.show()\n",
        "\n",
        "    # ~ area under curve using trapezoidal rule\n",
        "    return -np.trapz(FRs, x=FAs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcEP5cEZqUYl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cP_pFIsy5p2",
        "outputId": "0f146fb7-5733-4ab6-b161-3dab8fcfa0b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.energy = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        energy = self.energy(input)\n",
        "        alpha = torch.softmax(energy, dim=-2)\n",
        "        return (input * alpha).sum(dim=-2)\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, out_channels=config.cnn_out_channels,\n",
        "                kernel_size=config.kernel_size, stride=config.stride\n",
        "            ),\n",
        "            nn.Flatten(start_dim=1, end_dim=2),\n",
        "        )\n",
        "\n",
        "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
        "            config.stride[0] + 1\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.gru_num_layers,\n",
        "            dropout=0.1,\n",
        "            bidirectional=config.bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.attention = Attention(config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        input = input.unsqueeze(dim=1)\n",
        "        conv_output = self.conv(input).transpose(-1, -2)\n",
        "        gru_output, _ = self.gru(conv_output)\n",
        "        contex_vector = self.attention(gru_output)\n",
        "        output = self.classifier(contex_vector)\n",
        "        return output\n",
        "\n",
        "config = TaskConfig()\n",
        "model = CRNN(config)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DmmSFvWaqUYn"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, opt, loader, log_melspec, device):\n",
        "    model.train()\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UIeRbn4tqUYo"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validation(model, loader, log_melspec, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_losses, accs, FAs, FRs = [], [], [], []\n",
        "    all_probs, all_labels = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        output = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        all_probs.append(probs[:, 1].cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "        val_losses.append(loss.item())\n",
        "        accs.append(\n",
        "            torch.sum(argmax_probs == labels).item() /  # ???\n",
        "            torch.numel(argmax_probs)\n",
        "        )\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "\n",
        "    # area under FA/FR curve for whole loader\n",
        "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
        "    return au_fa_fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PpyvKwp0k3IU"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "history = defaultdict(list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSNW-nZCJ4Q0"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8sVpHNoocgA",
        "outputId": "143233a9-fdc9-4851-dbbf-85836b10b14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
            "    (1): Flatten(start_dim=1, end_dim=2)\n",
            "  )\n",
            "  (gru): GRU(144, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (attention): Attention(\n",
            "    (energy): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "config = TaskConfig(hidden_size=32)\n",
        "model = CRNN(config).to(config.device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zedXm9dmINAE",
        "outputId": "17e88c6b-c569-4fbf-ae1d-a23ee3f7baa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25387"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "sum([p.numel() for p in model.parameters()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "32oooz4lqUYo",
        "outputId": "e8c22872-993f-4486-c74a-0acd20afc0f3",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END OF EPOCH 19\n"
          ]
        }
      ],
      "source": [
        "# TRAIN\n",
        "\n",
        "for n in range(TaskConfig.num_epochs):\n",
        "\n",
        "    train_epoch(model, opt, train_loader,\n",
        "                melspec_train, config.device)\n",
        "\n",
        "    au_fa_fr = validation(model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "    history['val_metric'].append(au_fa_fr)\n",
        "\n",
        "    clear_output()\n",
        "    plt.plot(history['val_metric'])\n",
        "    plt.ylabel('Metric')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print('END OF EPOCH', n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBkTUHZcVugz",
        "outputId": "c71670be-5147-4f37-f983-d396d000200e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'val_metric': [0.0009220517443066985,\n",
              "              0.0004944739763455365,\n",
              "              0.00023872278966140286,\n",
              "              0.00018623522491164672,\n",
              "              0.00015326452452594922,\n",
              "              0.0001137295218010178,\n",
              "              8.857630874658972e-05,\n",
              "              6.681863026581993e-05,\n",
              "              6.966515046201499e-05,\n",
              "              5.783747115623174e-05,\n",
              "              5.3964532776083735e-05,\n",
              "              4.376599395994671e-05,\n",
              "              4.337213581959268e-05,\n",
              "              3.363906571478315e-05,\n",
              "              3.754780938041788e-05,\n",
              "              3.162800217994513e-05,\n",
              "              2.746862151590329e-05,\n",
              "              3.844294151758614e-05,\n",
              "              2.809521401192107e-05,\n",
              "              2.2915382711507416e-05]})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi #reproducible on Tesla T4 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKqCpk6pEAFu",
        "outputId": "8090f9ae-f68c-41ed-e492-0b25e4220444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  6 17:25:08 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    27W /  70W |    610MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# state = {\n",
        "#             \"arch\": type(model).__name__,\n",
        "#             \"epoch\": 20,\n",
        "#             \"state_dict\": model.state_dict(),\n",
        "#             \"optimizer\": opt.state_dict(),\n",
        "#             \"config\": config,\n",
        "#         }\n",
        "# torch.save(state, 'base_model.pth')"
      ],
      "metadata": {
        "id": "7Dw_SroYM9FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('base_model.pth', config.device)\n",
        "if checkpoint[\"config\"] != config:\n",
        "    print('different configs')\n",
        "loaded_model = CRNN(checkpoint[\"config\"]).to(config.device)\n",
        "if checkpoint[\"arch\"] != type(loaded_model).__name__:\n",
        "    print('different architectures')\n",
        "loaded_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NDvda6lPdrf",
        "outputId": "6155eb76-0074-41b3-ee6e-7472b656f096"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "different configs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation(loaded_model, val_loader, melspec_val, config.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtGWT3EUQlp5",
        "outputId": "8ec46ed9-ca68-4498-8e0e-f6585ec5b91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "102it [00:09, 10.48it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2915382711507416e-05"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading works fine"
      ],
      "metadata": {
        "id": "qaHweIV7R8Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation funtions"
      ],
      "metadata": {
        "id": "OlSNASUVqz6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGhR4vOzqzG5",
        "outputId": "8f66d2d4-5545-4252-fcd1-73ce86fd5ab8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from thop) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->thop) (4.1.1)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class Timer:\n",
        "\n",
        "    def __init__(self, name: str, verbose=False):\n",
        "        self.name = name\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.t = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.t = time.time() - self.t\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"{self.name.capitalize()} | Elapsed time : {self.t:.5f}\")\n",
        "\n",
        "from thop import profile  # !pip install thop\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.model = nn.Conv1d(1, 1, 3, bias=False)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "        \n",
        "# profile(Model(), (torch.randn(1, 1, 4), ))  # -> (6.0 MACs, 3.0 parameters)\n",
        "\n",
        "import tempfile\n",
        "\n",
        "def get_size_in_megabytes(model):\n",
        "    # https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#look-at-model-size\n",
        "    with tempfile.TemporaryFile() as f:\n",
        "        torch.save(model.state_dict(), f)\n",
        "        size = f.tell() / 2**20\n",
        "    return size"
      ],
      "metadata": {
        "id": "nyYTd9Ybrtjf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation run 1\n"
      ],
      "metadata": {
        "id": "z1NeSUV-fFiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 7\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "pvxbqzcnfIp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_config = TaskConfig()\n",
        "student_config = TaskConfig(cnn_out_channels=4, hidden_size=16, num_epochs=40)\n",
        "student_model = CRNN(student_config).to(student_config.device)\n",
        "student_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6TnMpsZfMAa",
        "outputId": "550fc8d0-0cde-4ba1-a933-1493b5878e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 4, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(72, 16, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_ce, alpha_mse = 1, 1\n",
        "n_params = sum([p.numel() for p in student_model.parameters()])\n",
        "wandb.init(\n",
        "    project='kws',\n",
        "    config = {\n",
        "        'alpha_ce': alpha_ce,\n",
        "        'alpha_mse': alpha_mse,\n",
        "        'n_params': n_params,\n",
        "        'student_config': student_config,\n",
        "        }\n",
        "    )\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=student_config.learning_rate,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)\n"
      ],
      "metadata": {
        "id": "c5ILuM_PfOO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('distilled_model_best.pth', student_config.device)\n",
        "if checkpoint[\"config\"] != student_config:\n",
        "    print('different configs')\n",
        "student_model = CRNN(checkpoint[\"config\"]).to(student_config.device)\n",
        "if checkpoint[\"arch\"] != type(student_model).__name__:\n",
        "    print('different architectures')\n",
        "student_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_myYtMV5kkS",
        "outputId": "a94caba9-5789-4bc0-a426-664c7e4f1bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "au_fa_fr = validation(student_model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "\n",
        "student_weight = get_size_in_megabytes(student_model)\n",
        "teacher_weight = get_size_in_megabytes(loaded_model)\n",
        "print(f'Quality of distilled model is: {au_fa_fr}, weight compression is: {student_weight/teacher_weight}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx4cRGBg5oAo",
        "outputId": "fff81d0f-53bc-4591-8782-8de877e7cfd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "102it [00:07, 13.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality of distilled model is: 2.8930670673278113e-05, weight compression is: 0.287269817378133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.randn(1, 3 * 16000)[...,:2 * 16000]\n",
        "batch = LogMelspec(is_train=False, config=TaskConfig)(batch.to(student_config.device))\n",
        "prof_student = profile(student_model, (batch, ))\n",
        "prof_teacher = profile(loaded_model, (batch, ))\n",
        "print(f'Student model: { 2 * prof_student[0] / 1e3}k FLOPs, {prof_student[1] / 1e3}k parameters' )\n",
        "print(f'Teacher model: {2 * prof_teacher[0] / 1e3}k FLOPs, {prof_teacher[1] / 1e3}k parameters' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJMATbgN5rcN",
        "outputId": "e688b3ed-9160-4d44-d1bd-a6d7c4bfd3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "Student model: 627.872k FLOPs, 6.679k parameters\n",
            "Teacher model: 1809.216k FLOPs, 25.387k parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Distillation run 2"
      ],
      "metadata": {
        "id": "CHarZXzAFtsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 7\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "WZqBdjYggVn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_config = TaskConfig()\n",
        "student_config = TaskConfig(cnn_out_channels=4, hidden_size=16, num_epochs=40, stride=(2, 10))\n",
        "student_model = CRNN(student_config).to(student_config.device)\n",
        "student_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn-75JGggZYN",
        "outputId": "4f2206a1-b017-41b3-b3a0-769c39420753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 4, kernel_size=(5, 20), stride=(2, 10))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(72, 16, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_ce, alpha_mse = 1, 1\n",
        "n_params = sum([p.numel() for p in student_model.parameters()])\n",
        "wandb.init(\n",
        "    project='kws',\n",
        "    config = {\n",
        "        'alpha_ce': alpha_ce,\n",
        "        'alpha_mse': alpha_mse,\n",
        "        'n_params': n_params,\n",
        "        'student_config': student_config,\n",
        "        }\n",
        "    )\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=student_config.learning_rate,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAPBFdT_gnEM",
        "outputId": "921c2b7d-9f2d-49ae-8aa3-eb8d574e26e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221106_123919-31is9hvt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/alishudi/kws/runs/31is9hvt\" target=\"_blank\">morning-haze-15</a></strong> to <a href=\"https://wandb.ai/alishudi/kws\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:42<00:00,  9.53it/s]\n",
            "102it [00:06, 15.45it/s]\n",
            "100%|██████████| 405/405 [00:37<00:00, 10.87it/s]\n",
            "102it [00:05, 18.42it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.38it/s]\n",
            "102it [00:05, 18.75it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.91it/s]\n",
            "102it [00:05, 18.34it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.08it/s]\n",
            "102it [00:05, 18.39it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.17it/s]\n",
            "102it [00:05, 18.01it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.13it/s]\n",
            "102it [00:05, 18.30it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.27it/s]\n",
            "102it [00:05, 18.26it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.30it/s]\n",
            "102it [00:05, 18.11it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.34it/s]\n",
            "102it [00:05, 18.37it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.32it/s]\n",
            "102it [00:05, 18.76it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.33it/s]\n",
            "102it [00:05, 18.15it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.25it/s]\n",
            "102it [00:05, 18.50it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.35it/s]\n",
            "102it [00:05, 18.07it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.40it/s]\n",
            "102it [00:05, 18.67it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.33it/s]\n",
            "102it [00:07, 12.75it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.44it/s]\n",
            "102it [00:05, 18.34it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.34it/s]\n",
            "102it [00:05, 18.55it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.33it/s]\n",
            "102it [00:05, 18.71it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.32it/s]\n",
            "102it [00:05, 18.56it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.37it/s]\n",
            "102it [00:05, 18.38it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.36it/s]\n",
            "102it [00:05, 18.44it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.45it/s]\n",
            "102it [00:05, 18.60it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.35it/s]\n",
            "102it [00:05, 18.52it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.34it/s]\n",
            "102it [00:05, 18.48it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.32it/s]\n",
            "102it [00:05, 18.20it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.39it/s]\n",
            "102it [00:05, 18.15it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.31it/s]\n",
            "102it [00:05, 18.53it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.34it/s]\n",
            "102it [00:05, 18.47it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.39it/s]\n",
            "102it [00:05, 18.17it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.36it/s]\n",
            "102it [00:05, 18.48it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.50it/s]\n",
            "102it [00:05, 18.29it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.38it/s]\n",
            "102it [00:05, 18.28it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.38it/s]\n",
            "102it [00:05, 18.20it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.25it/s]\n",
            "102it [00:05, 18.14it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.37it/s]\n",
            "102it [00:05, 18.46it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.36it/s]\n",
            "102it [00:05, 18.44it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.35it/s]\n",
            "102it [00:05, 18.41it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.36it/s]\n",
            "102it [00:05, 17.94it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.35it/s]\n",
            "102it [00:05, 18.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('distilled_model.pth', student_config.device)\n",
        "if checkpoint[\"config\"] != student_config:\n",
        "    print('different configs')\n",
        "student_model = CRNN(checkpoint[\"config\"]).to(student_config.device)\n",
        "if checkpoint[\"arch\"] != type(student_model).__name__:\n",
        "    print('different architectures')\n",
        "student_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23EI_dwzwSWO",
        "outputId": "93197015-bdf6-4ecb-a2e3-7f64c6ba1355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "au_fa_fr = validation(student_model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "\n",
        "student_weight = get_size_in_megabytes(student_model)\n",
        "teacher_weight = get_size_in_megabytes(loaded_model)\n",
        "print(f'Quality of distilled model is: {au_fa_fr}, weight compression is: {teacher_weight/student_weight}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3J-WGs7v9hY",
        "outputId": "7cbe2939-de87-4642-82e5-e56cc8c4cf45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality of distilled model is: 3.5477070369768646e-05, weight compression is: 3.2976475646459065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.randn(1, 3 * 16000)[...,:2 * 16000]\n",
        "batch = LogMelspec(is_train=False, config=TaskConfig)(batch.to(student_config.device))\n",
        "prof_student = profile(student_model, (batch, ))\n",
        "prof_teacher = profile(loaded_model, (batch, ))\n",
        "print(f'Student model: { 2 * prof_student[0] / 1e3}k FLOPs, {prof_student[1] / 1e3}k parameters' )\n",
        "print(f'Teacher model: {2 * prof_teacher[0] / 1e3}k FLOPs, {prof_teacher[1] / 1e3}k parameters' )\n",
        "print(f'Speedup: {prof_teacher[0] / prof_student[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeufqBRSwCa3",
        "outputId": "9dba418e-1c3b-4030-9bdb-8fa2d1454ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "Student model: 518.688k FLOPs, 6.679k parameters\n",
            "Teacher model: 1809.216k FLOPs, 25.387k parameters\n",
            "Speedup: 3.4880621876735147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With bigger stride model became bit faster, size is the same"
      ],
      "metadata": {
        "id": "2Zza6XBxERXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation 1 gru layer"
      ],
      "metadata": {
        "id": "INi-yK7QFydw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 7\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "student_config = TaskConfig()\n",
        "student_config = TaskConfig(cnn_out_channels=4, hidden_size=16, num_epochs=40, gru_num_layers=1)\n",
        "student_model = CRNN(student_config).to(student_config.device)\n",
        "student_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zmqJXM9F2Gm",
        "outputId": "0ac3783a-d236-404c-b08e-0591b19116ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 4, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(72, 16, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_ce, alpha_mse = 1, 1\n",
        "n_params = sum([p.numel() for p in student_model.parameters()])\n",
        "wandb.init(\n",
        "    project='kws',\n",
        "    config = {\n",
        "        'alpha_ce': alpha_ce,\n",
        "        'alpha_mse': alpha_mse,\n",
        "        'n_params': n_params,\n",
        "        'student_config': student_config,\n",
        "        }\n",
        "    )\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=student_config.learning_rate,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AUwthg7vFl-h",
        "outputId": "045268fd-11d1-459c-8efa-f78d13556894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:31is9hvt) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>au_fa_fr_val</td><td>█▅▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_ce</td><td>██▇▆▆▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_mse</td><td>█▇▆▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>au_fa_fr_val</td><td>4e-05</td></tr><tr><td>loss_ce</td><td>0.26499</td></tr><tr><td>loss_mse</td><td>0.65472</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">morning-haze-15</strong>: <a href=\"https://wandb.ai/alishudi/kws/runs/31is9hvt\" target=\"_blank\">https://wandb.ai/alishudi/kws/runs/31is9hvt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221106_123919-31is9hvt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:31is9hvt). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221106_131929-dkcitvo1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/alishudi/kws/runs/dkcitvo1\" target=\"_blank\">vibrant-oath-16</a></strong> to <a href=\"https://wandb.ai/alishudi/kws\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:34<00:00, 11.77it/s]\n",
            "102it [00:05, 17.54it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.69it/s]\n",
            "102it [00:05, 17.90it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.03it/s]\n",
            "102it [00:05, 17.88it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 11.97it/s]\n",
            "102it [00:05, 17.83it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.05it/s]\n",
            "102it [00:05, 17.64it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.06it/s]\n",
            "102it [00:05, 17.85it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.08it/s]\n",
            "102it [00:05, 17.50it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.00it/s]\n",
            "102it [00:05, 17.69it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.00it/s]\n",
            "102it [00:05, 17.97it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.09it/s]\n",
            "102it [00:05, 17.90it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.00it/s]\n",
            "102it [00:05, 17.78it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.21it/s]\n",
            "102it [00:05, 18.17it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.07it/s]\n",
            "102it [00:05, 17.79it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.14it/s]\n",
            "102it [00:05, 17.86it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.04it/s]\n",
            "102it [00:05, 17.80it/s]\n",
            "100%|██████████| 405/405 [00:38<00:00, 10.44it/s]\n",
            "102it [00:06, 15.35it/s]\n",
            "100%|██████████| 405/405 [00:36<00:00, 11.02it/s]\n",
            "102it [00:05, 18.06it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.86it/s]\n",
            "102it [00:05, 17.15it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.65it/s]\n",
            "102it [00:05, 17.61it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.87it/s]\n",
            "102it [00:05, 17.63it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.65it/s]\n",
            "102it [00:05, 17.59it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 11.95it/s]\n",
            "102it [00:05, 17.84it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.84it/s]\n",
            "102it [00:05, 17.52it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 11.92it/s]\n",
            "102it [00:05, 17.67it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.88it/s]\n",
            "102it [00:05, 17.61it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 11.94it/s]\n",
            "102it [00:05, 17.62it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.38it/s]\n",
            "102it [00:05, 17.42it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 11.99it/s]\n",
            "102it [00:05, 17.93it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.18it/s]\n",
            "102it [00:05, 18.13it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.37it/s]\n",
            "102it [00:05, 18.05it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.42it/s]\n",
            "102it [00:05, 18.35it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.49it/s]\n",
            "102it [00:05, 18.59it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.41it/s]\n",
            "102it [00:05, 18.74it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.41it/s]\n",
            "102it [00:05, 17.48it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.10it/s]\n",
            "102it [00:05, 18.07it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.25it/s]\n",
            "102it [00:05, 18.27it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.28it/s]\n",
            "102it [00:05, 18.44it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.52it/s]\n",
            "102it [00:05, 18.46it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.32it/s]\n",
            "102it [00:05, 18.41it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.38it/s]\n",
            "102it [00:05, 18.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('distilled_model.pth', student_config.device)\n",
        "if checkpoint[\"config\"] != student_config:\n",
        "    print('different configs')\n",
        "student_model = CRNN(checkpoint[\"config\"]).to(student_config.device)\n",
        "if checkpoint[\"arch\"] != type(student_model).__name__:\n",
        "    print('different architectures')\n",
        "student_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvDBiGiDIw1O",
        "outputId": "596c6691-4547-4d47-f8e3-5657587cc1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "au_fa_fr = validation(student_model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "\n",
        "student_weight = get_size_in_megabytes(student_model)\n",
        "teacher_weight = get_size_in_megabytes(loaded_model)\n",
        "print(f'Quality of distilled model is: {au_fa_fr}, weight compression is: {teacher_weight/student_weight}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R83VLm3MIzSe",
        "outputId": "89314e43-28cc-4716-ec99-64902f4c0c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "102it [00:06, 16.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality of distilled model is: 4.145058549847149e-05, weight compression is: 4.607113742652422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.randn(1, 3 * 16000)[...,:2 * 16000]\n",
        "batch = LogMelspec(is_train=False, config=TaskConfig)(batch.to(student_config.device))\n",
        "prof_student = profile(student_model, (batch, ))\n",
        "prof_teacher = profile(loaded_model, (batch, ))\n",
        "print(f'Student model: { 2 * prof_student[0] / 1e3}k FLOPs, {prof_student[1] / 1e3}k parameters' )\n",
        "print(f'Teacher model: {2 * prof_teacher[0] / 1e3}k FLOPs, {prof_teacher[1] / 1e3}k parameters' )\n",
        "print(f'Speedup: {prof_teacher[0] / prof_student[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EZL30spI3ZD",
        "outputId": "8fb8ef90-481a-43e7-c374-00210f20310a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "Student model: 547.648k FLOPs, 5.047k parameters\n",
            "Teacher model: 1809.216k FLOPs, 25.387k parameters\n",
            "Speedup: 3.3036110786490593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation stride 10 and hidden=12"
      ],
      "metadata": {
        "id": "m0cOsgorI4pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 7\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "student_config = TaskConfig()\n",
        "student_config = TaskConfig(cnn_out_channels=2, hidden_size=16, num_epochs=20, gru_num_layers=1)\n",
        "student_model = CRNN(student_config).to(student_config.device)\n",
        "student_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuWeCyY6KpQ3",
        "outputId": "f920ae80-812c-40dd-ca67-c44d783efa23"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 2, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(36, 16, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_ce, alpha_mse = 1, 2\n",
        "n_params = sum([p.numel() for p in student_model.parameters()])\n",
        "wandb.init(\n",
        "    project='kws',\n",
        "    config = {\n",
        "        'alpha_ce': alpha_ce,\n",
        "        'alpha_mse': alpha_mse,\n",
        "        'n_params': n_params,\n",
        "        'student_config': student_config,\n",
        "        }\n",
        "    )\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=student_config.learning_rate,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "MyOXE308OyhF",
        "outputId": "09f904fd-3d61-4c41-d5be-d8867d71955e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221106_172752-2p1he8kx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/alishudi/kws/runs/2p1he8kx\" target=\"_blank\">quiet-sun-26</a></strong> to <a href=\"https://wandb.ai/alishudi/kws\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:35<00:00, 11.42it/s]\n",
            "102it [00:05, 18.16it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.18it/s]\n",
            "102it [00:05, 18.20it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.76it/s]\n",
            "102it [00:05, 17.86it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.66it/s]\n",
            "102it [00:05, 18.01it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.72it/s]\n",
            "102it [00:05, 17.87it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.75it/s]\n",
            "102it [00:05, 17.69it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.83it/s]\n",
            "102it [00:05, 18.15it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.76it/s]\n",
            "102it [00:05, 18.02it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.77it/s]\n",
            "102it [00:05, 17.64it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.82it/s]\n",
            "102it [00:05, 18.12it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.02it/s]\n",
            "102it [00:05, 17.57it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.90it/s]\n",
            "102it [00:05, 18.27it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.83it/s]\n",
            "102it [00:05, 17.77it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.77it/s]\n",
            "102it [00:05, 17.88it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.80it/s]\n",
            "102it [00:05, 17.81it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.86it/s]\n",
            "102it [00:05, 17.81it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.88it/s]\n",
            "102it [00:05, 17.70it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 11.96it/s]\n",
            "102it [00:05, 18.45it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.37it/s]\n",
            "102it [00:05, 18.51it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.39it/s]\n",
            "102it [00:05, 18.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7za2vTYCCnTq",
        "outputId": "ac98f617-e74f-4733-9e4e-26d2604648bd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:33<00:00, 12.23it/s]\n",
            "102it [00:05, 17.94it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.40it/s]\n",
            "102it [00:05, 18.42it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.36it/s]\n",
            "102it [00:05, 18.36it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.72it/s]\n",
            "102it [00:05, 18.45it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.42it/s]\n",
            "102it [00:05, 18.36it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.71it/s]\n",
            "102it [00:06, 14.58it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.41it/s]\n",
            "102it [00:05, 18.39it/s]\n",
            "100%|██████████| 405/405 [00:39<00:00, 10.22it/s]\n",
            "102it [00:05, 18.51it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.20it/s]\n",
            "102it [00:05, 18.27it/s]\n",
            "100%|██████████| 405/405 [00:44<00:00,  9.19it/s]\n",
            "102it [00:07, 13.15it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.72it/s]\n",
            "102it [00:05, 17.87it/s]\n",
            "100%|██████████| 405/405 [00:36<00:00, 11.15it/s]\n",
            "102it [00:05, 17.76it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.71it/s]\n",
            "102it [00:05, 18.07it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.65it/s]\n",
            "102it [00:05, 17.57it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.62it/s]\n",
            "102it [00:05, 17.70it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.66it/s]\n",
            "102it [00:05, 17.72it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.61it/s]\n",
            "102it [00:05, 17.85it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.63it/s]\n",
            "102it [00:05, 17.77it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.62it/s]\n",
            "102it [00:05, 17.67it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.58it/s]\n",
            "102it [00:05, 17.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB4rARAbG6cg",
        "outputId": "b39f97f0-f61d-467c-f52f-fef09eea7454"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:35<00:00, 11.37it/s]\n",
            "102it [00:05, 17.97it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.71it/s]\n",
            "102it [00:05, 18.06it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.78it/s]\n",
            "102it [00:06, 15.97it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.03it/s]\n",
            "102it [00:05, 17.93it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.31it/s]\n",
            "102it [00:05, 17.84it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.17it/s]\n",
            "102it [00:05, 17.84it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.22it/s]\n",
            "102it [00:05, 18.33it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.18it/s]\n",
            "102it [00:05, 18.08it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.23it/s]\n",
            "102it [00:05, 18.32it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.19it/s]\n",
            "102it [00:05, 18.30it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.28it/s]\n",
            "102it [00:05, 18.26it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.34it/s]\n",
            "102it [00:05, 18.15it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.26it/s]\n",
            "102it [00:05, 17.98it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.33it/s]\n",
            "102it [00:05, 18.48it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.39it/s]\n",
            "102it [00:05, 18.38it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.26it/s]\n",
            "102it [00:05, 18.38it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.33it/s]\n",
            "102it [00:05, 18.08it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.21it/s]\n",
            "102it [00:05, 17.97it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.44it/s]\n",
            "102it [00:05, 18.67it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.25it/s]\n",
            "102it [00:05, 18.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=5e-5,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdyFhfw7LZY4",
        "outputId": "e5e2a4ee-0399-4f17-ce49-f0a3112f0f23"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:33<00:00, 12.06it/s]\n",
            "102it [00:05, 18.42it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 11.97it/s]\n",
            "102it [00:05, 18.01it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.88it/s]\n",
            "102it [00:05, 17.70it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.84it/s]\n",
            "102it [00:05, 18.19it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.81it/s]\n",
            "102it [00:05, 17.24it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.51it/s]\n",
            "102it [00:08, 12.22it/s]\n",
            "100%|██████████| 405/405 [00:53<00:00,  7.51it/s]\n",
            "102it [00:05, 18.11it/s]\n",
            "100%|██████████| 405/405 [00:40<00:00,  9.99it/s]\n",
            "102it [00:08, 12.62it/s]\n",
            "100%|██████████| 405/405 [00:43<00:00,  9.25it/s]\n",
            "102it [00:06, 15.95it/s]\n",
            "100%|██████████| 405/405 [00:38<00:00, 10.57it/s]\n",
            "102it [00:10,  9.99it/s]\n",
            "100%|██████████| 405/405 [00:44<00:00,  9.19it/s]\n",
            "102it [00:05, 18.08it/s]\n",
            "100%|██████████| 405/405 [00:45<00:00,  8.86it/s]\n",
            "102it [00:07, 13.60it/s]\n",
            "100%|██████████| 405/405 [00:39<00:00, 10.16it/s]\n",
            "102it [00:07, 13.92it/s]\n",
            "100%|██████████| 405/405 [00:52<00:00,  7.78it/s]\n",
            "102it [00:06, 14.77it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.15it/s]\n",
            "102it [00:05, 17.79it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.90it/s]\n",
            "102it [00:05, 17.84it/s]\n",
            "100%|██████████| 405/405 [00:40<00:00,  9.94it/s]\n",
            "102it [00:09, 10.31it/s]\n",
            "100%|██████████| 405/405 [00:52<00:00,  7.76it/s]\n",
            "102it [00:08, 11.64it/s]\n",
            "100%|██████████| 405/405 [00:38<00:00, 10.51it/s]\n",
            "102it [00:05, 17.29it/s]\n",
            "100%|██████████| 405/405 [00:41<00:00,  9.78it/s]\n",
            "102it [00:08, 12.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=5e-5,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32uxcbDXPxOY",
        "outputId": "8b85515c-681f-4f3a-84b9-3af134eade19"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:35<00:00, 11.43it/s]\n",
            "102it [00:05, 17.69it/s]\n",
            "100%|██████████| 405/405 [00:50<00:00,  7.99it/s]\n",
            "102it [00:08, 11.97it/s]\n",
            "100%|██████████| 405/405 [00:45<00:00,  8.99it/s]\n",
            "102it [00:07, 12.81it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.63it/s]\n",
            "102it [00:05, 17.89it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.43it/s]\n",
            "102it [00:05, 17.83it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.56it/s]\n",
            "102it [00:07, 13.99it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.56it/s]\n",
            "102it [00:05, 17.89it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.51it/s]\n",
            "102it [00:05, 17.74it/s]\n",
            "100%|██████████| 405/405 [00:36<00:00, 11.13it/s]\n",
            "102it [00:05, 17.73it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.59it/s]\n",
            "102it [00:05, 18.01it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.56it/s]\n",
            "102it [00:05, 17.57it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.63it/s]\n",
            "102it [00:05, 17.72it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.53it/s]\n",
            "102it [00:05, 17.52it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.64it/s]\n",
            "102it [00:05, 17.09it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.69it/s]\n",
            "102it [00:05, 17.83it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.65it/s]\n",
            "102it [00:05, 17.63it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.72it/s]\n",
            "102it [00:05, 17.50it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.62it/s]\n",
            "102it [00:05, 18.20it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.67it/s]\n",
            "102it [00:05, 17.92it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.77it/s]\n",
            "102it [00:05, 18.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=5e-5,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBcRmHztUg-B",
        "outputId": "9796929b-416c-4e14-e407-c49c3d0a4f25"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:32<00:00, 12.48it/s]\n",
            "102it [00:05, 18.24it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.54it/s]\n",
            "102it [00:05, 18.33it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.20it/s]\n",
            "102it [00:05, 18.82it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.33it/s]\n",
            "102it [00:05, 18.34it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.45it/s]\n",
            "102it [00:05, 18.32it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.54it/s]\n",
            "102it [00:05, 18.62it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.50it/s]\n",
            "102it [00:05, 18.38it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.51it/s]\n",
            "102it [00:05, 18.34it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.26it/s]\n",
            "102it [00:05, 18.50it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.71it/s]\n",
            "102it [00:05, 18.25it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.72it/s]\n",
            "102it [00:05, 17.62it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.72it/s]\n",
            "102it [00:05, 17.70it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.81it/s]\n",
            "102it [00:05, 17.72it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.63it/s]\n",
            "102it [00:08, 12.56it/s]\n",
            "100%|██████████| 405/405 [00:38<00:00, 10.43it/s]\n",
            "102it [00:05, 17.56it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.67it/s]\n",
            "102it [00:05, 17.97it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.71it/s]\n",
            "102it [00:05, 17.70it/s]\n",
            "100%|██████████| 405/405 [00:35<00:00, 11.37it/s]\n",
            "102it [00:05, 17.78it/s]\n",
            "100%|██████████| 405/405 [00:41<00:00,  9.67it/s]\n",
            "102it [00:07, 12.83it/s]\n",
            "100%|██████████| 405/405 [00:36<00:00, 11.25it/s]\n",
            "102it [00:05, 17.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=5e-5,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKNJClj0YHG9",
        "outputId": "9adb23fb-1cf7-4763-9408-cc30a2c2a73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:40<00:00, 10.03it/s]\n",
            "102it [00:05, 17.81it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.01it/s]\n",
            "102it [00:05, 18.18it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.73it/s]\n",
            "102it [00:05, 18.06it/s]\n",
            "100%|██████████| 405/405 [00:33<00:00, 12.14it/s]\n",
            "102it [00:05, 18.23it/s]\n",
            "100%|██████████| 405/405 [00:34<00:00, 11.82it/s]\n",
            "102it [00:05, 17.39it/s]\n",
            "100%|██████████| 405/405 [00:32<00:00, 12.48it/s]\n",
            "102it [00:05, 18.48it/s]\n",
            "100%|██████████| 405/405 [00:38<00:00, 10.61it/s]\n",
            "102it [00:10, 10.03it/s]\n",
            "100%|██████████| 405/405 [00:47<00:00,  8.52it/s]\n",
            "102it [00:05, 17.85it/s]\n",
            "100%|██████████| 405/405 [00:43<00:00,  9.41it/s]\n",
            "102it [00:05, 18.03it/s]\n",
            "100%|██████████| 405/405 [00:36<00:00, 11.05it/s]\n",
            "102it [00:11,  8.74it/s]\n",
            "100%|██████████| 405/405 [00:36<00:00, 11.06it/s]\n",
            "102it [00:05, 18.36it/s]\n",
            " 82%|████████▏ | 331/405 [00:27<00:06, 12.00it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('distilled_model_1.pth', student_config.device)\n",
        "if checkpoint[\"config\"] != student_config:\n",
        "    print('different configs')\n",
        "student_model = CRNN(checkpoint[\"config\"]).to(student_config.device)\n",
        "if checkpoint[\"arch\"] != type(student_model).__name__:\n",
        "    print('different architectures')\n",
        "student_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cvxr-WuctZb",
        "outputId": "89d53a77-bf1b-4eba-b1bc-18c7683faf95"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "different configs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_ce, alpha_mse = 1, 2\n",
        "n_params = sum([p.numel() for p in student_model.parameters()])\n",
        "wandb.init(\n",
        "    project='kws',\n",
        "    config = {\n",
        "        'alpha_ce': alpha_ce,\n",
        "        'alpha_mse': alpha_mse,\n",
        "        'n_params': n_params,\n",
        "        'student_config': student_config,\n",
        "        }\n",
        "    )\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    student_model.parameters(),\n",
        "    lr=5e-5,\n",
        "    weight_decay=student_config.weight_decay\n",
        ")\n",
        "\n",
        "loaded_model.eval()\n",
        "student_model.train()\n",
        "best_metric = 10\n",
        "\n",
        "for n in range(student_config.num_epochs):\n",
        "    student_model.train()\n",
        "    ce_losses, mse_losses = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        batch, labels = batch.to(student_config.device), labels.to(student_config.device)\n",
        "        batch = melspec_train(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = student_model(batch)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = loaded_model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss_ce = F.cross_entropy(logits, labels)\n",
        "        loss_mse = F.mse_loss(logits, teacher_logits)\n",
        "        ce_losses.append(loss_ce.item())\n",
        "        mse_losses.append(loss_mse.item())\n",
        "        \n",
        "        loss = alpha_ce * loss_ce + alpha_mse * loss_mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    \n",
        "    au_fa_fr = validation(student_model, val_loader,\n",
        "                        melspec_val, student_config.device)\n",
        "    \n",
        "    if au_fa_fr < best_metric:\n",
        "        best_metric = au_fa_fr\n",
        "        state = {\n",
        "            \"arch\": type(student_model).__name__,\n",
        "            \"state_dict\": student_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "        torch.save(state, 'distilled_model.pth')\n",
        "        wandb.save('distilled_model.pth')\n",
        "\n",
        "    metrics = {\n",
        "        'loss_ce': np.mean(ce_losses),\n",
        "        'loss_mse': np.mean(mse_losses),\n",
        "        'au_fa_fr_val': au_fa_fr\n",
        "        }\n",
        "    wandb.log(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "KGGolaYPcztt",
        "outputId": "8cd08c1b-915f-40e5-ef6e-174094d63057"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221106_193943-1bqi27a9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/alishudi/kws/runs/1bqi27a9\" target=\"_blank\">true-glitter-27</a></strong> to <a href=\"https://wandb.ai/alishudi/kws\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [01:04<00:00,  6.27it/s]\n",
            "102it [00:16,  6.16it/s]\n",
            "100%|██████████| 405/405 [01:07<00:00,  5.98it/s]\n",
            "102it [00:10, 10.03it/s]\n",
            "100%|██████████| 405/405 [01:04<00:00,  6.23it/s]\n",
            "102it [00:10,  9.42it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.33it/s]\n",
            "102it [00:10,  9.76it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.41it/s]\n",
            "102it [00:10,  9.75it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.42it/s]\n",
            "102it [00:10,  9.74it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.42it/s]\n",
            "102it [00:10,  9.66it/s]\n",
            "100%|██████████| 405/405 [01:02<00:00,  6.46it/s]\n",
            "102it [00:10,  9.80it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.41it/s]\n",
            "102it [00:10,  9.69it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.40it/s]\n",
            "102it [00:10,  9.70it/s]\n",
            "100%|██████████| 405/405 [01:05<00:00,  6.18it/s]\n",
            "102it [00:10,  9.76it/s]\n",
            "100%|██████████| 405/405 [01:02<00:00,  6.48it/s]\n",
            "102it [00:10,  9.78it/s]\n",
            "100%|██████████| 405/405 [01:02<00:00,  6.47it/s]\n",
            "102it [00:10,  9.61it/s]\n",
            "100%|██████████| 405/405 [01:02<00:00,  6.45it/s]\n",
            "102it [00:10,  9.75it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.34it/s]\n",
            "102it [00:10,  9.77it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.41it/s]\n",
            "102it [00:10,  9.72it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.42it/s]\n",
            "102it [00:10,  9.67it/s]\n",
            "100%|██████████| 405/405 [01:03<00:00,  6.41it/s]\n",
            "102it [00:10,  9.77it/s]\n",
            "100%|██████████| 405/405 [01:06<00:00,  6.11it/s]\n",
            "102it [00:10,  9.41it/s]\n",
            "100%|██████████| 405/405 [01:04<00:00,  6.32it/s]\n",
            "102it [00:10,  9.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('distilled_model.pth', student_config.device)\n",
        "if checkpoint[\"config\"] != student_config:\n",
        "    print('different configs')\n",
        "student_model = CRNN(checkpoint[\"config\"]).to(student_config.device)\n",
        "if checkpoint[\"arch\"] != type(student_model).__name__:\n",
        "    print('different architectures')\n",
        "student_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P6fLEVfUUeV",
        "outputId": "c3a66824-1271-47ba-cb56-a543b9d222db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "au_fa_fr = validation(student_model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "\n",
        "student_weight = get_size_in_megabytes(student_model)\n",
        "teacher_weight = get_size_in_megabytes(loaded_model)\n",
        "print(f'Quality of distilled model is: {au_fa_fr}, weight compression is: {teacher_weight/student_weight}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr_nLPgNUVyo",
        "outputId": "214350ce-42d9-4ee9-bbf4-ac4dc5d333ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "102it [00:10,  9.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality of distilled model is: 7.762585893523137e-05, weight compression is: 6.626898318855072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.randn(1, 3 * 16000)[...,:2 * 16000]\n",
        "batch = LogMelspec(is_train=False, config=TaskConfig)(batch.to(student_config.device))\n",
        "prof_student = profile(student_model, (batch, ))\n",
        "prof_teacher = profile(loaded_model, (batch, ))\n",
        "print(f'Student model: { 2 * prof_student[0] / 1e3}k FLOPs, {prof_student[1] / 1e3}k parameters' )\n",
        "print(f'Teacher model: {2 * prof_teacher[0] / 1e3}k FLOPs, {prof_teacher[1] / 1e3}k parameters' )\n",
        "print(f'Speedup: {prof_teacher[0] / prof_student[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbfZka0TUWHV",
        "outputId": "88fbe4a5-5de0-493a-84a4-faaa3f0b59e2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "Student model: 302.56k FLOPs, 3.117k parameters\n",
            "Teacher model: 1809.216k FLOPs, 25.387k parameters\n",
            "Speedup: 5.979693283976732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#quantization"
      ],
      "metadata": {
        "id": "gf3XkGAdEBqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Во время прошлого экспа квота на гпу в колабе кончилась, теперь воспроизводимость поломается"
      ],
      "metadata": {
        "id": "wSM3oMNbm3YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 7\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "distilled_config = TaskConfig()\n",
        "distilled_config = TaskConfig(cnn_out_channels=4, hidden_size=16, num_epochs=40, gru_num_layers=1)\n",
        "distilled_model = CRNN(distilled_config).to(distilled_config.device)\n",
        "distilled_model\n",
        "\n",
        "checkpoint = torch.load('distilled_stride10_model.pth', distilled_config.device)\n",
        "if checkpoint[\"config\"] != distilled_config:\n",
        "    print('different configs')\n",
        "distilled_model = CRNN(checkpoint[\"config\"]).to(distilled_config.device)\n",
        "if checkpoint[\"arch\"] != type(distilled_model).__name__:\n",
        "    print('different architectures')\n",
        "for key in [\"total_ops\", \"total_params\", \"conv.1.total_ops\", \"conv.1.total_params\", \"attention.total_ops\", \"attention.total_params\", \"attention.energy.1.total_ops\", \"attention.energy.1.total_params\"]:\n",
        "    del checkpoint['state_dict'][key] #в последний момент сломалась загрузка \n",
        "\n",
        "distilled_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpVfw3uDmAAo",
        "outputId": "cfca80cc-be3b-47e2-f2e0-0c8cddf06310"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "different configs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "au_fa_fr = validation(distilled_model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "\n",
        "distilled_weight = get_size_in_megabytes(distilled_model)\n",
        "teacher_weight = get_size_in_megabytes(loaded_model)\n",
        "print(f'Quality of distilled model is: {au_fa_fr}, weight compression is: {teacher_weight/distilled_weight}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtKvShLPnRjH",
        "outputId": "c36eea3b-35af-4a05-9754-60a37bd22fec"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "102it [00:10,  9.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality of distilled model is: 5.577866724074474e-05, weight compression is: 3.4755132763204255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Было меньше 5.5е-5, есть логи на вандб("
      ],
      "metadata": {
        "id": "wJ_yRNMXniyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization"
      ],
      "metadata": {
        "id": "ZbCw_IAOEFcu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_model = torch.quantization.quantize_dynamic(distilled_model, dtype=torch.float16)"
      ],
      "metadata": {
        "id": "R9OendzbETLf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "au_fa_fr = validation(fp16_model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "\n",
        "fp16_weight = get_size_in_megabytes(fp16_model)\n",
        "teacher_weight = get_size_in_megabytes(loaded_model)\n",
        "print(f'Quality of fp16 quantized model is: {au_fa_fr}, weight compression is: {teacher_weight/fp16_weight}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHnuJ-qdF5g_",
        "outputId": "ed84bf4f-c9f5-4503-f616-4aa8bb2866b5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "102it [00:12,  8.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality of fp16 quantized model is: 5.579060233590698e-05, weight compression is: 3.201705467528252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\n",
        "            \"arch\": type(fp16_model).__name__,\n",
        "            \"state_dict\": fp16_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "torch.save(state, 'fp16_model.pth')"
      ],
      "metadata": {
        "id": "jVsbf0j6NeWB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qint8_model = torch.quantization.quantize_dynamic(distilled_model, dtype=torch.qint8)"
      ],
      "metadata": {
        "id": "CJLllB7fGEzt"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "au_fa_fr = validation(qint8_model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "\n",
        "qint8_weight = get_size_in_megabytes(qint8_model)\n",
        "teacher_weight = get_size_in_megabytes(loaded_model)\n",
        "print(f'Quality of qint8 quantized model is: {au_fa_fr}, weight compression is: {teacher_weight/qint8_weight}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQgHZGJEGMer",
        "outputId": "b1da1082-48a6-4cce-f7cd-629b6640f151"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "102it [00:10,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality of qint8 quantized model is: 5.692443637632013e-05, weight compression is: 6.8490184939721015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\n",
        "            \"arch\": type(qint8_model).__name__,\n",
        "            \"state_dict\": qint8_model.state_dict(),\n",
        "            \"config\": student_config,\n",
        "        }\n",
        "torch.save(state, 'qint8_model.pth')"
      ],
      "metadata": {
        "id": "HxeSuQbuNObC"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Качество после кавнтизации почти не падает, от qint8 сжатие усилилось двукратно, если бы не проблема с воспроизводимостью наверно норм модели были бы"
      ],
      "metadata": {
        "id": "6SElvGeKrQO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plots\n"
      ],
      "metadata": {
        "id": "86KZ4EyGMd20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = {\n",
        "    'metric': [],\n",
        "    'FLOPs': [],\n",
        "    'memory': [],\n",
        "    'name': []\n",
        "        }\n",
        "\n",
        "# for name in ['qint8', 'fp16', 'base', 'distilled_stride10']:\n",
        "#     checkpoint = torch.load(name + '_model.pth')\n",
        "#     if name != 'base':\n",
        "#         conf = distilled_config\n",
        "#     else:\n",
        "#         conf = config\n",
        "#     loaded = CRNN(checkpoint[\"config\"]).to(conf.device)\n",
        "#     loaded = loaded.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "#     weight  = get_size_in_megabytes(loaded)\n",
        "#     au_fa_fr = validation(loaded, val_loader,\n",
        "#                           melspec_val, conf.device)\n",
        "#     prof = profile(loaded, (batch, ))\n",
        "\n",
        "#     data['name'].append(name)\n",
        "#     data['memory'].append(weight)\n",
        "#     data['metric'].append(au_fa_fr)\n",
        "#     data['FLOPs'].append(2 * prof[0])\n",
        "\n",
        "# checkpoint = torch.load('base_model.pth', map_location=torch.device('cpu'))\n",
        "# conf = config\n",
        "# loaded = CRNN(checkpoint[\"config\"]).to(distilled_config.device)\n",
        "# loaded = loaded.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# weight  = get_size_in_megabytes(loaded)\n",
        "# au_fa_fr = validation(loaded, val_loader,\n",
        "#                         melspec_val, conf.device)\n",
        "# prof = profile(loaded, (batch, ))\n",
        "\n",
        "# data['name'].append('base_model')\n",
        "# data['memory'].append(weight)\n",
        "# data['metric'].append(au_fa_fr)\n",
        "# data['FLOPs'].append(2 * prof[0])\n",
        "\n",
        "checkpoint = torch.load('distilled_stride10_model.pth')\n",
        "conf = distilled_config\n",
        "loaded = CRNN(checkpoint[\"config\"]).to(conf.device)\n",
        "loaded = loaded.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "weight  = get_size_in_megabytes(loaded)\n",
        "au_fa_fr = validation(loaded, val_loader,\n",
        "                        melspec_val, conf.device)\n",
        "prof = profile(loaded, (batch, ))\n",
        "\n",
        "data['name'].append('distilled_model')\n",
        "data['memory'].append(weight)\n",
        "data['metric'].append(au_fa_fr)\n",
        "data['FLOPs'].append(2 * prof[0])\n",
        "\n",
        "checkpoint = torch.load('qint8_model.pth')\n",
        "conf = distilled_config\n",
        "loaded = CRNN(checkpoint[\"config\"]).to(conf.device)\n",
        "loaded = loaded.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "weight  = get_size_in_megabytes(loaded)\n",
        "au_fa_fr = validation(loaded, val_loader,\n",
        "                        melspec_val, conf.device)\n",
        "prof = profile(loaded, (batch, ))\n",
        "\n",
        "data['name'].append('qint8_model')\n",
        "data['memory'].append(weight)\n",
        "data['metric'].append(au_fa_fr)\n",
        "data['FLOPs'].append(2 * prof[0])\n",
        "\n",
        "checkpoint = torch.load('fp16_model.pth')\n",
        "conf = distilled_config\n",
        "loaded = CRNN(checkpoint[\"config\"]).to(conf.device)\n",
        "loaded = loaded.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "weight  = get_size_in_megabytes(loaded)\n",
        "au_fa_fr = validation(loaded, val_loader,\n",
        "                        melspec_val, conf.device)\n",
        "prof = profile(loaded, (batch, ))\n",
        "\n",
        "data['name'].append('fp16_model')\n",
        "data['memory'].append(weight)\n",
        "data['metric'].append(au_fa_fr)\n",
        "data['FLOPs'].append(2 * prof[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "TRgp03U9GW0v",
        "outputId": "50c3cc92-e89a-4662-cb45-50e036958365"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-ae630dc9b920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# data['FLOPs'].append(2 * prof[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilled_stride10_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilled_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             dtype=dtype)\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    137\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(data = data, x='metric', y = 'flops')"
      ],
      "metadata": {
        "id": "NOFqwz4jQTpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(data = data, x='metric', y = 'memory')"
      ],
      "metadata": {
        "id": "RjNoVhoTQVw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "gEXZu1rPWzqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNN_streaming():\n",
        "\n",
        "    def __init__(self, config: TaskConfig, model: CRNN):\n",
        "        self.window_length = config.max_window_length\n",
        "        self.hidden = None\n",
        "        self.buffer = torch.zeros([1, 1, config.n_mels, 0]).to(config.device)\n",
        "        self.gru_buffer = torch.zeros([1, 0, config.hidden_size]).to(config.device)\n",
        "        self.model = model\n",
        "        \n",
        "    \n",
        "    def forward(self, input):\n",
        "        with torch.inference_mode():\n",
        "            input = input.unsqueeze(dim=1)\n",
        "            # print(self.buffer.shape, input.shape)\n",
        "            self.buffer = torch.cat([self.buffer, input], dim = -1)\n",
        "            if self.buffer.shape[-1] > self.window_length:\n",
        "                self.buffer = self.buffer[:,:,:,-self.window_length:]\n",
        "            # print(self.buffer.shape)\n",
        "            if self.buffer.shape[-1] < config.kernel_size[1]:\n",
        "                # print('buffer is smaller than receptive field')\n",
        "                return 0\n",
        "            \n",
        "            conv_output = self.model.conv(self.buffer).transpose(-1, -2)\n",
        "            gru_output, self.hidden = self.model.gru(conv_output, self.hidden)\n",
        "            # print(self.gru_buffer.shape, gru_output.shape)\n",
        "            self.gru_buffer = torch.cat([self.gru_buffer, gru_output], dim = 1)\n",
        "            if self.gru_buffer.shape[1] > self.window_length:\n",
        "                self.gru_buffer = self.gru_buffer[:,1:,:]\n",
        "            contex_vector = self.model.attention(self.gru_buffer)\n",
        "            output = self.model.classifier(contex_vector)\n",
        "        return output\n",
        "\n",
        "class CRNN_streaming_full():\n",
        "    #version with argmax and melspec for saving\n",
        "    def __init__(self, config: TaskConfig, model: CRNN):\n",
        "        self.window_length = config.max_window_length\n",
        "        self.hidden = None\n",
        "        self.buffer = torch.zeros([1, 1, config.n_mels, 0]).to(config.device)\n",
        "        self.gru_buffer = torch.zeros([1, 0, config.hidden_size]).to(config.device)\n",
        "        self.model = model\n",
        "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_fft=400,\n",
        "                win_length=400,\n",
        "                hop_length=160,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "    def call_melspec(self, batch):\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = self.call_melspec(input)\n",
        "        with torch.inference_mode():\n",
        "            input = input.unsqueeze(dim=1)\n",
        "            # print(self.buffer.shape, input.shape)\n",
        "            self.buffer = torch.cat([self.buffer, input], dim = -1)\n",
        "            if self.buffer.shape[-1] > self.window_length:\n",
        "                self.buffer = self.buffer[:,:,:,-self.window_length:]\n",
        "            # print(self.buffer.shape)\n",
        "            if self.buffer.shape[-1] < config.kernel_size[1]:\n",
        "                # print('buffer is smaller than receptive field')\n",
        "                return 0\n",
        "            \n",
        "            conv_output = self.model.conv(self.buffer).transpose(-1, -2)\n",
        "            gru_output, self.hidden = self.model.gru(conv_output, self.hidden)\n",
        "            # print(self.gru_buffer.shape, gru_output.shape)\n",
        "            self.gru_buffer = torch.cat([self.gru_buffer, gru_output], dim = 1)\n",
        "            if self.gru_buffer.shape[1] > self.window_length:\n",
        "                self.gru_buffer = self.gru_buffer[:,1:,:]\n",
        "            contex_vector = self.model.attention(self.gru_buffer)\n",
        "            output = self.model.classifier(contex_vector)\n",
        "        return F.softmax(output[0], dim=0)[1].item()"
      ],
      "metadata": {
        "id": "YQ6OUfYxW8tg"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = []\n",
        "i = 0\n",
        "while len(zeros) < 20:\n",
        "    x, y = val_set[i]['wav'], val_set[i]['label']\n",
        "    if y == 0:\n",
        "        zeros.append(x)\n",
        "    i += 1\n",
        "\n",
        "sheila = []\n",
        "while len(sheila) != 1:\n",
        "    x, y = val_set[i]['wav'], val_set[i]['label']\n",
        "    if y == 1:\n",
        "        sheila.append(x)\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "17k7NL2sYMdv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = []\n",
        "for x in zeros[:10] + sheila + zeros[10:]:\n",
        "    x = melspec_val(x)\n",
        "    batch.append(x)"
      ],
      "metadata": {
        "id": "zO-eb6odyOqm"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.cat(batch, dim = 1)\n",
        "batch = batch.unsqueeze(dim=0)"
      ],
      "metadata": {
        "id": "rwbUlP1gyo_a"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS3roKnKzlg_",
        "outputId": "3f9b49f4-b288-4c8f-9d0d-699152f77864"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 40, 2121])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape[2] / 21"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNbZjb9x4ibG",
        "outputId": "089fe1a7-6c99-461f-d469-dce64430676a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101.0"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streaming = CRNN_streaming(distilled_config, distilled_model)\n",
        "outputs = []\n",
        "\n",
        "for i in range(0, batch.shape[2], distilled_config.stride[1]):\n",
        "    output = streaming.forward(batch[:, :, i:i+distilled_config.stride[1]])\n",
        "    if type(output) == torch.Tensor:\n",
        "        outputs.append(F.softmax(output[0], dim=0)[1].item())\n",
        "    else:\n",
        "        outputs.append(0)"
      ],
      "metadata": {
        "id": "LLchJ9gzydJV"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "6a8LHpDs2tM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(data=outputs)\n",
        "plt.vlines([batch.shape[2] / 21 * 10 / distilled_config.stride[1], batch.shape[2] / 21 * 11 / distilled_config.stride[1]], ymin=0, ymax=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "jBu2BLrj2gxR",
        "outputId": "49c61f68-850d-4a2f-fb24-eefdabad1818"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.LineCollection at 0x7f603777f610>"
            ]
          },
          "metadata": {},
          "execution_count": 136
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Небольшая задержка + ложное срабатывание в конце"
      ],
      "metadata": {
        "id": "hSiombFg5H_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# streaming_full = CRNN_streaming_full(distilled_config, distilled_model)\n",
        "m = torch.jit.script(CRNN_streaming_full(distilled_config, distilled_model))\n",
        "m.save(\"streaming.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "msnF5nx3-afs",
        "outputId": "0e686b6f-eb21-4514-ebdf-d6e9e58df187"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-e16983f759d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# streaming_full = CRNN_streaming_full(distilled_config, distilled_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCRNN_streaming_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistilled_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"streaming.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_script_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_class\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mrcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateResolutionCallbackForClassMethods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;31m# Script the type of obj if it hasn't already been scripted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0m_compile_and_register_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqualified_class_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0mclass_ty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_python_cu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_class_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create an empty torch._C.ScriptObject with the scripted type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36m_compile_and_register_class\u001b[0;34m(obj, rcb, qualified_name)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscript_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_jit_class_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrontend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_args_for_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mscript_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_script_class_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/frontend.py\u001b[0m in \u001b[0;36mget_jit_class_def\u001b[0;34m(cls, self_name)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mproperties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_class_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0msourcelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_source_lines_and_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorReport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourcelines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mdedent_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdedent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_sources.py\u001b[0m in \u001b[0;36mget_source_lines_and_file\u001b[0;34m(obj, error_msg)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# in case getsourcefile throws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0msourcelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \"\"\"\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/package/package_importer.py\u001b[0m in \u001b[0;36mpatched_getfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_package_imported_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_orig_getfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is a built-in class'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <class '__main__.CRNN_streaming_full'> is a built-in class"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из за своей кривой реализации (не унаследовался) не могу сохранить стриминговую модель"
      ],
      "metadata": {
        "id": "8HBpJ4gbBBcg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_lhrn5O-qUYZ",
        "812GwLfqqUYf",
        "KA1gPmE1h9pI",
        "CcEP5cEZqUYl",
        "OlSNASUVqz6P",
        "z1NeSUV-fFiT",
        "CHarZXzAFtsC",
        "INi-yK7QFydw",
        "m0cOsgorI4pV",
        "86KZ4EyGMd20"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('ASR_HW')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "575dc77c0d6a0a1586307ec96332040943146d58a3aa0edf2d9b9dca61255cc7"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}